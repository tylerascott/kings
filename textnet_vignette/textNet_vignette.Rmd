---
title: "textNet_vignette"
author: "E Zufall and T Scott"
date: "2023-09-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

While text-based network extraction is typically based on co-occurrence of words within documents, there are limited tools for generating directed networks based on the syntactic relationships between entities within a sentence, for arbitrarily long documents. Since many official documents studied in the social and environmental sciences, such as government planning documents or bills, consist of thousands of pages, this type of replicable, automated network extraction, is extremely valuable. The textNet package is designed to enable just this. The textNet package facilitates the automated analysis and comparison of many documents, based on their respective network characteristics. It is flexible enough that any desired entity categories, such as organizations, geopolitical entities, or dates, can be preserved.  

## Extracted Entity Network

The directed network generated by textNet represents the collection of all identified entities in the document, joined by edges signifying the verbs that connect them. The user can specify which entity categories should be preserved. The output format is a list containing four data.tables: an edgelist, a nodelist, a verblist, and an appositive list. 

The edgelist includes edge attributes such as verb tense, any auxiliary verbs in the verb phrase, whether an open clausal complement (Universal Dependencies code "xcomp") is associated with the primary verb, whether any hedging words were detected in the sentence, and whether any negations were detected in the sentence. 

The returned edgelist by default contains both complete and incomplete edges. A complete edge includes a source, verb, and target. An incomplete edge includes either a source or a target, but not both, along with its associated verb. Incomplete edges convey information about which entities are commonly associated with different verbs, even though they do not reveal information about which other entities they are linked to in the network. These incomplete edges can be filtered out when converting the output into a network object, such as through the network package or the igraph package. The nodelist returns all entities of the desired types found in the document, regardless of whether they were found in the edgelist. Thus, the nodelist allows the presence of isolates to be documented, as well as preserving node attributes. The verblist includes all of the verbs found in the document, along with verb attributes imported from VerbNet. This can be used to conduct analyses of certain verb classifications of interest. Finally, the appositive list is a table of entities that may be synonyms. This list is generated from entities whose universal dependency parsing labels as appositives, and whose head token points to another entity. These pairs are included in the table as potential synonyms. If this feature is used, cleaning and filtering by hand is recommended, as appositives can at times be misidentified by existing NLP tools. An automated alternative we recommend is our find_acronym tool, which scans the entire document for acronyms defined parenthetically in-text and compiles them in a table.

This network is directed such that the entities that form the subject of the sentence are denoted as the "source" nodes, and the remaining entities are denoted as the "target" nodes. To identify whether each entity is a "source" or a "target", we use dependency parsing in the Universal Dependencies format, in which each token in a given sentence has an associated "syntactic head" token from which it is derived. Starting with each entity in the sentence, the chain of syntactic head tokens is traced back until either a subject or a verb is reached. If it reaches a subject first, the entity is considered a "source." If it reaches a verb first, it is considered a "target." 

To identify the subject, we search for the presence of at least one of the following subject tags: “nsubj” (nominal subject), “nsubjpass” (nominal subject – passive), “csubj” (clausal subject), “csubjpass” (clausal subject – passive), “agent”, and “expl” (expletive). To identify the object, we search for the presence of at least one of the following: “pobj” (object of preposition), “iobj” (indirect object), “dative”, “attr” (attribute), “dobj” (direct object), “oprd” (object predicate), “ccomp” (clausal complement), “xcomp” (open clausal complement), “acomp” (adjectival complement), or “pcomp” (complement of preposition).

If a subject token is reached first ("nsubj," "nsubjpass," "csubj," "csubjpass," "agent," or "expl"), this indicates that the original token is doing the verb action. That is, it serves some function related to the subject of the sentence. We designate this by tagging it “source,” since these types of relationships will be used to designate the “from” or “source” nodes in our directed network. If a verb token is reached first ("VERB" or "AUX"), this indicates that the verb action is occurring for or towards the original token, which we denote with the tag “target.” These tokens are potential “to” or “target” nodes in our directed network. Linking the two nodes is an edge representing the verb that connects them in the sentence. 

Due to the presence of tables, lists, or other anomalies in the original document, it is possible that a supposed “sentence” has a head token trail that does not lead to a verb as is normatively the case. In these instances, the tokens whose trails terminate with a non-subject, non-verb token are assigned neither “source” nor “target” tags. Finally, an exception is made if an appositive token is reached first, since this indicates that the token in question is merely a synonym or restatement of an entity that is already described elsewhere in the sentence and, accordingly, should not be treated as a separate node. Tokens that lead to appositives are assigned neither “source” nor “target” tags, but are preserved as a separate appositive list. 

If a verb phrase in the edgelist does not have any sources, the sources associated with the head token of the verb phrase's main verb (that is, the verb phrase's parent verb) are adopted as sources of that verb phrase. As of Version 1.0, textNet does not do this recursively, to preserve performance optimization.

The textNet::textnet_extract() function returns the full list of open clausal complement lemmas associated with the main verb as an edge attribute: "xcomp_verb". The list of auxiliary verbs and their corresponding lemmas associated with the main verb, as well as the list of auxiliary verbs and corresponding lemmas associated with the open clausal complements linked to the main verb, are also included as edge attributes: "helper_token", "helper_lemma", "xcomp_helper_token", and "xcomp_helper_lemma", respectively.

The extraction function also detects hedging words and negations. The function textNet::textnet_extract() produces an edge attribute "has_hedge", which is T if there is a hedging auxiliary verb ("may","might","can","could") or main verb ("seem","appear","suggest","tend","assume","indicate","estimate","doubt","believe") in the verb phrase.

Tense is also detected. The six tenses tagged by Spacy in textNet::parse_text() are preserved by textNet::textnet_extract() as an edge attribute "head_verb_tense". This attribute can take on one of six values: "VB" (verb, base form), "VBD" (verb, past tense), "VBG" (verb, gerund or present participle), "VBN" (verb, past participle), "VBP" (verb, non-3rd person singular present), or "VBZ" (verb, 3rd person singular present). Additionally, an edge attribute "is_future" is generated by textNet::textnet_extract(), which is T if the verb phrase contains an xcomp, has the token "going" as a head verb, and a being verb token as an auxiliary verb (i.e. is of the form "going to <verb>") or contains one of the following auxiliary verbs: "shall","will","wo", or "'ll" (i.e. is of the form "will <verb>").


## How to Use textNet

The following example will walk through the steps of using the textNet package to generate a "before" and "after" network of organizations and people discussed in the Gravelly Ford Water District Groundwater Sustainability Plan, before and after it underwent revisions required by the California Department of Water Resources. 

### Step 1: Process PDFs

This is a wrapper for pdftools, that has the option of using pdf_text or OCR. A header/footer removal tool is also included. This tool is solely based on carriage returns in the first or last few lines of the document, so can sometimes remove portions of paragraphs. However, not removing headers or footers can lead to improper inclusion of header and footer material in sentences, artificially inflating the presence of nodes whose entity names are included in the header and footer. Therefore, the header/footer remover is included by default but can be turned off if the user has a preferred header/footer removal tool to use.

```{r pdf_clean}
   library(textNet)
   pdfs <- c(list.files(path = ".", pattern = "old.pdf", full.names = T), 
          list.files(path = ".", pattern = "new.pdf", full.names = T))
   ocr <- F
   maxchar <- 10000

   old_new_text <- textNet::pdf_clean(pdfs, keep_pages=T, ocr=F, maxchar=10000, 
                     export_paths=NULL, return_to_memory=T, suppressWarn = F)
   names(old_new_text) <- c("old","new")
```

### Step 2: Parse Text

This wrapper for spacyR produces a table that can be fed into the textnet_extract function. Another NLP tool may be used to generate a similar table, as long as the output conforms to spacy tagging standards: Universal Dependencies tags for part of speech and Penn Treebank tags for tags. 

The textnet_extract function expects the parsed table to follow specific conventions. First, a row must be included for each token. The column names expected are doc_id (a unique ID for each page), sentence_id (a unique ID for each sentence), token_id (a unique ID for each token), token (a string that is the token, generally a word), lemma (the canonical or dictionary form of the token), pos (a code referring to the token's part of speech, according to http://universaldependencies.org/u/pos/), tag (a code referring to the token's part of speech, according to Penn Treebank https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), head_token_id (a numeric ID referring to the token_id of the head token of the current row's token), dep_rel (the dependency label according to https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md), and entity (entity type category defined by https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf, followed by "_B" if it is the first token in the entity or "_I" otherwise).

The example below demonstrates how to supplement the Named Entity Recognition capabilities of spacyR with a custom list of entities. This supplementation could be used to ensure that specific known entities are recognized; for instance, spacy might not detect that a consulting firm such as "Schmidt and Associates" is one entity rather than two. Conversely, this capability could be leveraged to create a new category of entities to detect, that a pretrained model is not designed to specifically recognize. For instance, to create a public health network, one might include a known list of contaminants and diseases and designate custom entity type tags for them, such as "CONTAM" and "DISEASE"). In this example, we investigate the connections between the organizations, people, and geopolitical entities discussed in the plan with the flow of water in the basin. To assist with this, we have input a custom list of known water bodies in the region and given it the entity designation "WATER". This is carried out by setting the variable "phrases_to_concatenate" to a character vector, including all of the custom entities. Then, the entity type can be set to the desired category. Note that this function is case-sensitive.

```{r parse}
   ret_path <- "/Users/elisemiller/miniconda3/envs/spacy_condaenv/bin/python"
   pages <- unlist(old_new_text)
   file_ids <- unlist(sapply(1:length(old_new_text), function(q) rep(names(old_new_text[q]),length(old_new_text[[q]]))))

   water <- c("surface water", "Surface water", "groundwater", "Groundwater", "San Joaquin River", "Cottonwood Creek", "Chowchilla Canal Bypass", "Friant Dam", "Sack Dam", "Friant Canal", "Chowchilla Bypass", "Fresno River", "Sacramento River", "Merced River","Chowchilla River", "Bass Lake", "Crane Valley Dam", "Willow Creek", "Millerton Lake", "Mammoth Pool", "Dam 6 Lake", "Delta","Tulare Lake", "Madera-Chowchilla canal", "lower aquifer", "upper aquifer", "upper and lower aquifers", "lower and upper aquifers", "Lower aquifer", "Upper aquifer", "Upper and lower aquifers", "Lower and upper aquifers")

   water_phrases <- water[stringr::str_detect(water,"\\s")]
   old_new_parsed <- textNet::parse_text(ret_path, 
                                 keep_hyph_together = F, 
                                 phrases_to_concatenate = water_phrases, 
                                 concatenator = "_", 
                                 pages, 
                                 file_ids, 
                                 parsed_filenames=c("old_parsed","new_parsed"), 
                                 parse_from_file = F, 
                                 overwrite = T)

   water_concat <- stringr::str_replace_all(water,"\\s","_")

#we don't want to overwrite the entity type. For instance, the word "Groundwater" 
#may just be part of an organization called a "Groundwater Sustainability Agency"
   old_new_parsed <- lapply(1:length(old_new_parsed), function (i){
      old_new_parsed[[i]][old_new_parsed[[i]]$token %in% water_concat & old_new_parsed[[i]]$entity=="",]$entity <- "WATER_B"
      return(old_new_parsed[[i]])
})

```

### Step 3: Extract Networks

The textnet_extract() function then extracts the entity network. It reads in the result of parse_text() as above or another parsing tool with appropriate column names and tagging conventions. The resulting object consists of a nodelist, an edgelist, a verblist, and a list of appositives. The nodelist variables are entity_name, the concatenated name of the entity; entity_type, which is a preservation from the entity_type attribute from the output of textNet::parse_text(); and num_appearances, which is the number of times the entity appears in the PDF text. (This is not the same as node degree, since there may be multiple edges, or if keep_incomplete_edges is set to false, no edges resulting from a single appearance of the entity in the document.)  The entity_type attribute represents Spacy's determination of entity type using its NER recognition, or if a custom parser or NER tool is used, the textnet_extract() function will preserve these entity type designations.

The file is saved to the provided filename, if provided. It is returned to memory if return_to_memory is set to T. At least one of these return pathways must be established to avoid an error. In this example, we only keep entity types in the nodelist, edgelist, and appositivelist that are listed under keep_entities; namely, "ORG", "GPE", "PERSON", and "WATER".

```{r extract}
   extracts <- vector(mode="list",length=length(old_new_parsed))
   for(m in 1:length(old_new_parsed)){
      extracts[[m]] <- textnet_extract(old_new_parsed[[m]],concatenator="_",cl=4,
                   keep_entities = c('ORG','GPE','PERSON','WATER'), 
                   return_to_memory=T, keep_incomplete_edges=T)
   }
```

### Step 4: Consolidate Entity Synonyms
In a document, the same real-world entity may be referenced in multiple ways. For instance, the document may introduce an organization using its full name, then use an acronym for the remainder of the document. To have more reliable network results, it is important to consolidate nodes that represent different naming conventions into a single node. The textNet package comes with a built-in tool for finding acronyms defined parenthetically within the text. This can be run on the result of pdf_clean() to generate a table with one column for acronyms and another for the corresponding full names, such that each row is a different instance of a phrase for which an acronym was detected. The use of find_acronyms() is demonstrated below.

```{r acronyms}
   old_acronyms <- find_acronyms(old_new_text[[1]])
   new_acronyms <- find_acronyms(old_new_text[[2]])
   
   print(head(old_acronyms))
   
```

The resulting table of acronyms can then be fed into a disambiguation tool, the textNet function disambiguate(). This tool is very flexible, allowing a user-defined custom vector or list of strings representing the original entity name to search for in the textnet_extract object, and another user-defined custom vector or list of strings representing the entity name to which to convert those instances. Additional inputs that may be useful here are names and abbreviations of known federal and state or regional agencies, or other entities that are likely to be discussed in the particular type of document being analyzed. There may also be topic-specific words or phrases that are likely to be discussed in the document. For instance, in Groundwater Sustainability Plans, it is common to discuss entities that involve the term "subbasin," but the spelling of this is not always consistent. 

In the example below, we define a "from" vector that includes the acronyms found through the previous step, as well as non-standard spellings of "subbasin." This function is case sensitive, so we have included two alternate cases that are likely to appear in the dataset. The "to" vector includes the full names from the find_acronyms result, along with the standard spelling of "subbasin". 

There are a few rules about the "from" and "to" columns. First, the length of "from" and "to" must be identical, since from[[i]] is replaced with to[[i]]. Second, there may not be any duplicated terms in the "from" list, since each string must be matched to a single replacement without ambiguity. It is acceptable to have duplicated terms in the "to" list. 

The "from" argument may be formatted as either a vector or a list. However, if it is a list, no element may contain more than one string. Each element in the list must be a single character vector. This is not the case for the "to" argument. The "match_partial_entity" argument defaults to F for each element of "from" and "to." However, it can be set to T or F for each individual element.  Replacing an acronym with its full name may only be wise if the entire name of the node is that acronym. Otherwise "EPA" could accidentally match on "NEPAL" and create a nonsense entity called "NEnvironmental_Protection_AgencyL". The risk of this for modern, sentence-case documents is decreased, as disambiguate() is intentionally a case-sensitive function. For the example below, we set match_partial_entity to F for each of the acronymns, but to T for the word "Sub_basin," since "Sub_basin" may very well be a portion of a longer entity, for which we would want to standardize the spelling.

The textnet_extract argument of disambiguate() accepts the result of the textnet_extract() function. The object returned by disambiguate() updates the edgelist\$source column, edgelist\$target column, and nodelist\$entity_name column to reflect the new node names.

There may be some cases in which one would want to convert a single node into multiple nodes, each preserving the original node's edges to other nodes. For instance, suppose a legal document refers to "The_Defendants" as a shorthand for referring to three individuals involved in the case. In the network, it may be desirable for these individuals to be represented as their own separate nodes, especially if the network is to be merged with those resulting from other documents, where the three defendants may be named separately. To convert this single node into multiple nodes that preserve all of their original edges to other entities, from[[j]] should be set to "The_Defendants", and to[[j]] should be set to a string vector including the individuals' names, such as c("John_Doe", "Jane_Doe", "Emily_Doe").

The package resolves loops such as from = c("hello","world"); to = c("world","hello") automatically, with a warning summarizing the rows that were removed. The default behavior is to loop through the disambiguation recursively, though by setting recursive to F, this can be overridden. The difference can be seen in the following example. Suppose that the following from list and to list are defined: from = c("MA","Mass"); to = c("Mass","Massachusetts"). If recursive = F, all instances of MA in the original textnet_extract object would be set to Mass, and all instances of Mass in the original textnet_extract object would be set to Massachusetts. If recursive = T, all instances of MA and Mass in the original textnet_extract object would be set to Massachusetts. The ability to toggle this behavior can be useful when concatenating a large from and to list based on multiple sources.

Information about the optional argument try_drop can be found in the package documentation.

```{r consolidate}
tofrom <- data.table::data.table(from = c(as.list(old_acronyms$acronym),
             list("Sub_basin","Sub_Basin",
             "upper_and_lower_aquifers",
             "Upper_and_lower_aquifers",
             "Lower_and_upper_aquifers",
             "lower_and_upper_aquifers")))
tofrom$to <- c(as.list(old_acronyms$name),
             list("Subbasin","Subbasin"),
             list(c("upper_aquifer","lower_aquifer")),
             list(c("upper_aquifer","lower_aquifer")),
             list(c("upper_aquifer","lower_aquifer")),
             list(c("upper_aquifer","lower_aquifer")))

   old_extract_clean <- disambiguate(
      from = tofrom$from, 
      to = tofrom$to,
      match_partial_entity = c(rep(F,length(old_acronyms$acronym)),T,T,F,F,F,F), 
      textnet_extract = extracts[[1]])
   
   new_extract_clean <- disambiguate(
      from = c(as.list(new_acronyms$acronym),
               list("Sub_basin","Sub_Basin",
                    "upper_and_lower_aquifers",
                    "Upper_and_lower_aquifers",
                    "Lower_and_upper_aquifers",
                    "lower_and_upper_aquifers")), 
      to = c(as.list(new_acronyms$name),
             list("Subbasin","Subbasin"),
             list(c("upper_aquifer","lower_aquifer")),
             list(c("upper_aquifer","lower_aquifer")),
             list(c("upper_aquifer","lower_aquifer")),
             list(c("upper_aquifer","lower_aquifer"))), 
      match_partial_entity = c(rep(F,length(new_acronyms$acronym)),T,T,F,F,F,F), 
      textnet_extract = extracts[[2]])

```

### Step 5: Get Network Attributes

A tool that generates an igraph or network object from the textnet_extract output is included in the package as the function export_to_network(). It returns a list that contains the igraph or network itself as the first element, and an attribute table as the second element. Functions from the sna, igraph, and network packages are invoked to create a network attribute table of common network-level attributes; see package documentation for details. 

```{r export}
   old_extract_net <- export_to_network(old_extract_clean, "igraph", keep_isolates = F, collapse_edges = F, self_loops = T)
   new_extract_net <- export_to_network(new_extract_clean, "igraph", keep_isolates = F, collapse_edges = F, self_loops = T)

   table <- t(format(rbind(old_extract_net[[2]], new_extract_net[[2]]), digits = 3, scientific = F))
   colnames(table) <- c("old","new")
   print(table)
```
The ggraph package has been used to create the two network visualizations seen here, using a weighted version of the igraphs constructed below.

```{r plot}
   library(ggraph)
   old_extract_plot <- export_to_network(old_extract_clean, "igraph", keep_isolates = T, collapse_edges = T, self_loops = T)[[1]]
   new_extract_plot <- export_to_network(new_extract_clean, "igraph", keep_isolates = T, collapse_edges = T, self_loops = T)[[1]]
   #order of these layers matters
   ggraph(old_extract_plot, layout = 'fr')+
      geom_edge_fan(aes(alpha = weight),
                    end_cap = circle(1,"mm"),
                    color = "#000000",
                    width = 0.3,
                    arrow = arrow(angle=15,length=unit(0.07,"inches"),ends = "last",type = "closed"))+
      #from Paul Tol's bright color scheme
      scale_color_manual(values = c("#4477AA","#228833","#CCBB44","#66CCEE"))+
      geom_node_point(aes(color = entity_type), size = 1,
                      alpha = 0.8)+
      labs(title= "Old Network")+
      theme_void()

   #order of these layers matters
   ggraph(new_extract_plot, layout = 'fr')+
      geom_edge_fan(aes(alpha = weight),
                    end_cap = circle(1,"mm"),
                    color = "#000000",
                    width = 0.3,
                    arrow = arrow(angle=15,length=unit(0.07,"inches"),ends = "last",type = "closed"))+
      #from Paul Tol's bright color scheme
      scale_color_manual(values = c("#4477AA","#228833","#CCBB44","#66CCEE"))+
      geom_node_point(aes(color = entity_type), size = 1,
                      alpha = 0.8)+
      labs(title= "New Network")+
      theme_void()
   
```

### Step 6: Find Top Features

Calculates the most common verbs and entities across the entire corpus of documents.

```{r top}
   top_feats <- top_features(list(old_extract_net[[1]], new_extract_net[[1]]))
   print(head(top_feats[[1]],10))
   print(head(top_feats[[2]],10))
```

### Step 7: Generate Composite Network

The combine_networks function allows a composite network to be generated from multiple export_to_network() outputs. This function is useful for understanding and analyzing the overlaps between the network of multiple documents. In this example, a composite network is not as useful, since these documents are not from two different regions being discussed but rather are two versions of the same document. However, for illustration purposes, the composite network is generated below. 

For best results, composite network generation should not be done without an adequate disambiguation in Step 4. A function is included that merges the edgelists and nodelists of all documents. If the same node name is mentioned in multiple documents, the node attributes associated with the highest total number of edges for that node name are preserved.

```{r composite}
   composite_net <- combine_networks(list(old_extract_net[[1]], new_extract_net[[1]]), mode = "weighted")
   ggraph(composite_net, layout = 'fr')+
      geom_edge_fan(aes(alpha = weight),
                    end_cap = circle(1,"mm"),
                    color = "#000000",
                    width = 0.3,
                    arrow = arrow(angle=15,length=unit(0.07,"inches"),ends = "last",type = "closed"))+
      #from Paul Tol's bright color scheme
      scale_color_manual(values = c("#4477AA","#228833","#CCBB44","#66CCEE"))+
      geom_node_point(aes(color = entity_type), size = 1,
                      alpha = 0.8)+
      labs(title= "New Network")+
      theme_void()
```


### Step 8: Explore Node Attributes

The network objects generated from export_to_network can be used to analyze the node attributes of the graphs. Below we calculate node-level attributes on a weighted version of the networks. We can also include the variable num_graphs_in from our composite network to investigate what kinds of nodes are found in both plans. For instance, the boxplots below show the rate at which each entity type is found in both plans.
```{r nodes}
library(network)
library(igraph)
    composite_tbl <- igraph::get.data.frame(composite_net, what = "vertices")
    composite_tbl <- composite_tbl[,c("name","num_graphs_in")]
    old_tbl <- igraph::get.data.frame(old_extract_net[[1]], what = "both")
    old_tbl$vertices <- dplyr::left_join(old_tbl$vertices, composite_tbl)
    old_net <- network::network(x=old_tbl$edges[,1:2], directed = T,
                          hyper = F, loops = T, multiple = T, 
                          bipartiate = F, vertices = old_tbl$vertices,
                          matrix.type = "edgelist")
    old_mat <- as.matrix(as.matrix(export_to_network(old_extract_clean, "igraph", keep_isolates = F, collapse_edges = T, self_loops = F)[[1]]))
    paths2 <- diag(old_mat %*% old_mat)
    recip <- 2*paths2 / sna::degree(old_net)
    totalCC <- DirectedClustering::ClustF(old_mat, type = "directed", isolates="zero")$totalCC
    closens <- sna::closeness(old_net, gmode = "graph", cmode="undirected")
    between <- sna::betweenness(old_net,gmode = "graph",cmode="undirected")
    deg <- sna::degree(old_net, gmode = "graph", cmode = "undirected")
    old_node_df <- dplyr::tibble(name = network::get.vertex.attribute(old_net, "vertex.names"), 
                       closens, 
                       between, 
                       deg,
                       recip,
                       totalCC,
                       entity_type = network::get.vertex.attribute(old_net,"entity_type"),
                       num_graphs_in = network::get.vertex.attribute(old_net, "num_graphs_in"))
    
    new_tbl <- igraph::get.data.frame(new_extract_net[[1]], what = "both")
    new_tbl$vertices <- dplyr::left_join(new_tbl$vertices, composite_tbl)
    new_net <- network::network(x=new_tbl$edges[,1:2], directed = T,
                          hyper = F, loops = T, multiple = T, 
                          bipartiate = F, vertices = new_tbl$vertices,
                          matrix.type = "edgelist")
    new_mat <- as.matrix(as.matrix(export_to_network(new_extract_clean, "igraph", keep_isolates = F, collapse_edges = T, self_loops = F)[[1]]))
    paths2 <- diag(new_mat %*% new_mat)
    recip <- 2*paths2 / sna::degree(new_net)
    totalCC <- DirectedClustering::ClustF(new_mat, type = "directed", isolates="zero")$totalCC
    closens <- sna::closeness(new_net, gmode = "graph", cmode="undirected")
    between <- sna::betweenness(new_net,gmode = "graph",cmode="undirected")
    deg <- sna::degree(new_net, gmode = "graph", cmode = "undirected")
    new_node_df <- dplyr::tibble(name = network::get.vertex.attribute(new_net, "vertex.names"), 
                       closens, 
                       between, 
                       deg,
                       recip,
                       totalCC,
                       entity_type = network::get.vertex.attribute(new_net,"entity_type"),
                       num_graphs_in = network::get.vertex.attribute(new_net, "num_graphs_in"))
    
    library(gridExtra)
    library(ggplot2)
    b1 <- ggplot(old_node_df, aes(x = entity_type, y = num_graphs_in)) + geom_boxplot() + theme_bw() + labs(title="Old Network")
    b2 <- ggplot(new_node_df, aes(x = entity_type, y = num_graphs_in)) + geom_boxplot() + theme_bw() + labs(title="New Network")
    grid.arrange(b1, b2, ncol=2)
```

### Potential Analyses

The network-level attributes output from export_to_network can also be analyzed against exogenous metadata that has been collected separately by the researcher regarding the different documents and their real-world context. The extracted networks, with their collections of verb attributes, node attributes, edge incidences, and edge attributes, can also be analyzed through a variety of tools, such as an ergm, to determine the probability of edge formation under certain conditions. A tergm could also shed light on the changes of a document over time, such as the multiple versions of the groundwater sustainability plan in this example. 



