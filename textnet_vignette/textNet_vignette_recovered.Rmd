---
title: "textNet_vignette"
author: "E Zufall and T Scott"
date: "2023-09-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

While tools exist to generate networks based on co-occurrence of words within documents (such as the "textnets" package), there are limited tools for generating directed networks based on the syntactic relationships between entities within a sentence, for arbitrarily long documents. As many official documents, such as government planning documents or bills such as those studied in the social sciences, consist of thousands of pages, this type of replicable, automated network extraction, is extremely valuable. This package, textNet, is designed to enable just this. The textNet package facilitates the automated analysis and comparison of many documents, based on their respective network characteristics. It is flexible enough that any desired categor(ies) of entity, such as organizations, geopolitical entities, or dates, can be preserved.  

```{r cars}
summary(cars)
```

## Extracted Entity Network

The directed network generated by textNet represents the collection of all identified entities in the document, joined by edges signifying the verbs that connect them. The user can specify which categories of entities should be preserved. The output format is a list containing four data.tables: an edgelist, a nodelist, a verblist, and an appositive list. 

The edgelist includes edge attributes such as verb tense, any auxiliary verbs in the verb phrase, whether an open clausal complement (Universal Dependencies code "xcomp") is associated with the primary verb, whether any hedging words were detected in the sentence, and whether any negations were detected in the sentence. The edgelist by default returns both complete and incomplete edges. A complete edge includes a source, verb, and target. An incomplete edge includes either a source or a target, but not both, along with its associated verb. Incomplete edges convey information about which entities are commonly associated with different verbs, even though they do not reveal information about which other entities they are linked to in the network. These incomplete edges can be filtered out when converting the output into a network object, such as through the network package or the igraph package. The nodelist returns all entities of the desired types found in the document, regardless of whether they were found in the edgelist. Thus, the nodelist allows the presence of isolates to be documented. The verblist includes all of the verbs found in the document, along with verb attributes imported from VerbNet. This can be used to conduct analyses of certain verb classifications of interest. Finally, the appositive list is a table of entities that may be synonyms. This list is generated from entities whose universal dependency parsing labels as appositives, and whose head token points to another entity. These pairs are included in the table as potential synonyms. If this feature is used, cleaning and filtering by hand is recommended, as appositives can at times be misidentified by NLP tools such as spacy. An automated alternative we recommend is our find_acronym tool, which scans the entire document for acronyms defined parenthetically in-text and compiles them in a table.

This network is directed such that the entities that form the subject of the sentence are denoted as the "source" nodes, and the remaining entities are denoted as the "target" nodes. To identify whether each entity is a "source" or a "target", we use dependency parsing in the Universal Dependencies format, in which each token in a given sentence has an associated "syntactic head" token from which it is derived. Starting with each entity in the sentence, the chain of syntactic head tokens is traced back until either a subject or a verb is reached. If it reaches a subject first, the entity is considered a "source." If it reaches a verb first, it is considered a "target." 

To identify the subject, we search for the presence of at least one of the following subject tags: “nsubj” (nominal subject), “nsubjpass” (nominal subject – passive), “csubj” (clausal subject), “csubjpass” (clausal subject – passive), “agent”, and “expl” (expletive). To identify the object, we search for the presence of at least one of the following: “pobj” (object of preposition), “iobj” (indirect object), “dative”, “attr” (attribute), “dobj” (direct object), “oprd” (object predicate), “ccomp” (clausal complement), “xcomp” (open clausal complement), “acomp” (adjectival complement), or “pcomp” (complement of preposition).

If a subject token is reached first ("nsubj," "nsubjpass," "csubj," "csubjpass," "agent," or "expl"), this indicates that the original token is doing the verb action. That is, it serves some function related to the subject of the sentence. We designate this by tagging it “source,” since these types of relationships will be used to designate the “from” or “source” nodes in our directed network. If a verb token is reached first ("VERB" or "AUX"), this indicates that the verb action is occurring for or towards the original token, which we denote with the tag “target.” These tokens are potential “to” or “target” nodes in our directed network. Linking the two nodes is an edge representing the verb that connects them in the sentence. Due to the presence of tables, lists, or other anomalies in the dataset, it is possible that the supposed “sentence” has a head token trail that does not lead to a verb as is normatively the case. In these instances, the tokens whose trails terminate with a non-subject, non-verb token are assigned neither “source” nor “target” tags. Finally, an exception is made if an appositive token is reached first, since this indicates that the token in question is merely a synonym or restatement of an entity that is already described elsewhere in the sentence and should not be treated as a separate node. Tokens that lead to appositives are assigned neither “source” nor “target” tags, but are preserved as a separate appositive list. 

If a verb phrase in the edgelist does not have any sources, the sources associated with the head token of the verb phrase's main verb (that is, the verb phrase's parent verb) are adopted as sources of that verb phrase. As of Version 1.0, textNet does not do this recursively, to preserve performance optimization.

The textNet::textnet_extract() function returns the full list of open clausal complement lemmas associated with the main verb as an edge attribute: "xcomp_verb". The list of auxiliary verbs and their corresponding lemmas associated with the main verb, as well as the list of auxiliary verbs and corresponding lemmas associated with the open clausal complements linked to the main verb, are also included as edge attributes: "helper_token", "helper_lemma", "xcomp_helper_token", and "xcomp_helper_lemma", respectively.

The extraction function also detects hedging words and negations. The function textNet::textnet_extract() produces an edge attribute "has_hedge", which is T if there is a hedging auxiliary verb ("may","might","can","could") or main verb ("seem","appear","suggest","tend","assume","indicate","estimate","doubt","believe") in the verb phrase.

Tense is also detected. The six tenses tagged by Spacy in textNet::parse() are preserved by textNet::textnet_extract() as an edge attribute "head_verb_tense". This attribute can take on one of six values: "VB" (verb, base form), "VBD" (verb, past tense), "VBG" (verb, gerund or present participle), "VBN" (verb, past participle), "VBP" (verb, non-3rd person singular present), or "VBZ" (verb, 3rd person singular present). Additionally, an edge attribute "is_future" is generated by textNet::textnet_extract(), which is T if the verb phrase contains an xcomp, has the token "going" as a head verb, and a being verb token as an auxiliary verb (i.e. is of the form "going to <verb>") or contains one of the following auxiliary verbs: "shall","will","wo", or "'ll" (i.e. is of the form "will <verb>").


## How to Use textNet

The following example will walk through the steps of using the textNet package to generate a "before" and "after" network of organizations and people discussed in the Gravelly Ford Water District Groundwater Sustainability Plan, before and after it underwent revisions required by the California Department of Water Resources. 

### Step 1: Process PDFs

This is a wrapper for pdftools, that has the option of using pdf_text or OCR. A header/footer removal tool is also included. This tool is solely based on carriage returns in the first or last few lines of the document, so can sometimes remove portions of paragraphs. However, not removing headers or footers can lead to improper inclusion of header and footer material in sentences, artificially inflating the presence of nodes whose entity names are included in the header and footer. Therefore, the header/footer remover is included by default but can be turned off if the user has a preferred header/footer removal tool to use.

```{r pdf_clean, include=FALSE}

pdfs <- c(list.files(path = ".", pattern = "old.pdf", full.names = T), 
          list.files(path = ".", pattern = "new.pdf", full.names = T))
ocr <- F
maxchar <- 10000

old_new_text <- textNet::pdf_clean(pdfs, keep_pages=T, ocr=F, maxchar=10000, export_paths=NULL, return_to_memory=T, suppressWarn = F)
names(old_new_text) <- c("old","new")
```

### Step 2: Parse Text

This is a wrapper for spacyR, but another NLP tool can be used as long as the output conforms to spacy tagging standards: Universal Dependencies tags for part of speech, Penn Treebank tags for tags. The textnet_extract function expects a row for each token. The column names expected by textnet_extract are doc_id (a unique ID for each page), sentence_id (a unique ID for each sentence), token_id (a unique ID for each token), token (a string that is the token, generally a word), lemma (the canonical or dictionary form of the token), pos (a code referring to the token's part of speech, according to http://universaldependencies.org/u/pos/), tag (a code referring to the token's part of speech, according to Penn Treebank https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), head_token_id (a numeric ID referring to the token_id of the head token of the current row's token), dep_rel (the dependency label according to https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md), and entity (entity type category defined by https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf, followed by "_B" if it is the first token in the entity or "_I" otherwise).

```{r parse, include=FALSE}
library(textNet)
ret_path <- "/Users/elisemiller/miniconda3/envs/spacy_condaenv/bin/python"
pages <- unlist(old_new_text)
file_ids <- unlist(sapply(1:length(old_new_text), function(q) rep(names(old_new_text[q]),length(old_new_text[[q]]))))
old_new_parsed <- textNet::parse(ret_path, 
                                 keep_hyph_together = F, 
                                 phrases_to_concatenate = NA, 
                                 concatenator = "_", 
                                 pages, 
                                 file_ids, 
                                 parsed_filenames=c("old_parsed","new_parsed"), 
                                 parse_from_file = F, 
                                 cl= 1, 
                                 overwrite = T)
```

### Step 3: Extract Networks

The textnet_extract() function then extracts the entity network. It reads in the result of parse() or another parsing tool with appropriate column names and tagging conventions. The resulting object consists of a nodelist, an edgelist, a verblist, and a list of appositives. The nodelist variables are entity_cat, the concatenated name of the entity; entity_type, which is a preservation from the entity_type attribute from the output of textNet::parse(); and num_appearances, which is the number of times the entity appears in the PDF text. (This is not the same as node degree, since there may be multiple edges, or if keep_incomplete_edges is set to false, no edges resulting from a single appearance of the entity in the document.)  The entity_type attribute represents Spacy's determination of entity type using its NER recognition, or if a custom parser or NER tool is used, the textnet_extract() function will preserve these entity type designations.

The file is saved to the provided filename, if provided. It is returned to memory if return_to_memory is set to T. At least one of these return pathways must be established to avoid an error. In this example, we only keep entity types in the nodelist, edgelist, and appositivelist that are listed under keep_entities; namely, "ORG", "GPE", and "PERSON".

```{r extract, include=FALSE}
nodeedge_filenames <- c("old_extract","new_extract")
for(m in 1:length(old_new_parsed)){
   textnet_extract(old_new_parsed[[m]],concatenator="_",file = nodeedge_filenames[m],cl=4,
                   keep_entities = c('ORG','GPE','PERSON'), 
                   return_to_memory=F, keep_incomplete_edges=T)
}
```

### Step 4: Consolidate Entity Synonyms
In a document, the same real-world entity may be referenced in multiple ways. For instance, the document may introduce an organization using its full name, then use an acronym for the remainder of the document. To have more reliable network results, it is important to consolidate nodes that represent different naming conventions into a single node. The textNet package comes with a built-in tool for finding acronyms defined parenthetically within the text. This can be run on the result of pdf_clean() to generate a table with one column for acronyms and another for the corresponding full names, such that each row is a different instance of a phrase for which an acronym was detected. The use of find_acronyms() is demonstrated below.

```{r acronyms, include=FALSE}
   old_acronyms <- find_acronyms(old_new_text[[1]])
   new_acronyms <- find_acronyms(old_new_text[[2]])
   
```

The resulting table of acronyms can then be fed into a disambiguation tool. This tool is very flexible, allowing a user-defined custom vector or list of strings representing the original entity name to search for in the textnet_extract object, and another user-defined custom vector or list of strings representing the entity name to which to convert those instances. Additional inputs that may be useful here are names and abbreviations of known federal and state or regional agencies, or other entities that are likely to be discussed in the particular type of document being analyzed. There may also be topic-specific words or phrases that are likely to be discussed in the document. For instance, in Groundwater Sustainability Plans, it is common to discuss entities that involve the term "subbasin," but the spelling of this is not always consistent. 

In the example below, we define a "from" vector that includes the acronyms found through the previous step, as well as non-standard spellings of "subbasin." This function is case sensitive, so we have included two alternate cases that are likely to appear in the dataset. The "to" vector includes the full names from the find_acronyms result, along with the standard spelling of "subbasin". 

There are a few rules about the "from" and "to" columns. First, the length of "from" and "to" must be identical, since from[[i]] is replaced with to[[i]]. Second, there may not be any duplicated terms in the "from" list, since each string must be matched to a single replacement without ambiguity. It is acceptable to have duplicated terms in the "to" list. 

The "from" argument may be formatted as either a vector or a list. However, if it is a list, no element may contain more than one string. Each element in the list must be a single character vector. This is not the case for the "to" argument. The "match_partial_entity" argument defaults to F for each element of "from" and "to." However, it can be set to T or F for each individual element.  Replacing an acronym with its full name may only be wise if the entire name of the node is that acronym. Otherwise "EPA" could accidentally match on "NEPAL" and create a nonsense entity called "NEnvironmental_Protection_AgencyL". The risk of this for modern, sentence-case documents is decreased, as disambiguate() is intentionally a case-sensitive function. For the example below, we set match_partial_entity to F for each of the acronymns, but to F for the word "Sub_basin," since "Sub_basin" may very well be a portion of a longer entity, for which we would want to standardize the spelling.

The textnet_extract argument accepts the result of the textnet_extract() function. The object returned by disambiguate() updates the edgelist\$source column, edgelist\$target column, and nodelist\$entity_cat column to reflect the new node names.

There may be some cases in which one would want to convert a single node into multiple nodes, each preserving the original node's edges to other nodes. For instance, suppose a legal document refers to "The_Defendants" as a shorthand for referring to three individuals involved in the case. In the network, it may be desirable for these individuals to be represented as their own separate nodes, especially if the network is to be merged with those resulting from other documents, where the three defendants may be named separately. To convert this single node into multiple nodes that preserve all of their original edges to other entities, from[[j]] should be set to "The_Defendants", and to[[j]] should be set to a string vector including the individuals' names, such as c("John_Doe", "Jane_Doe", "Emily_Doe").

The package resolves loops such as from = c("hello","world"); to = c("world","hello") automatically, with a warning summarizing the rows that were removed. The default behavior is to loop through the disambiguation recursively, though by setting recursive to F, this can be overridden. The difference can be seen in the following example. Suppose that the following from list and to list are defined: from = c("MA","Mass"); to = c("Mass","Massachusetts"). If recursive = F, all instances of MA in the original textnet_extract object would be set to Mass, and all instances of Mass in the original textnet_extract object would be set to Massachusetts. If recursive = T, all instances of MA and Mass in the original textnet_extract object would be set to Massachusetts. The ability to toggle this behavior can be useful when concatenating a large from and to list based on multiple sources.

Information about the optional argument try_drop can be found in the package documentation.

```{r consolidate, include = FALSE}
   old_extract_clean <- disambiguate(textnet_extract = readRDS("./old_extract"), from = c(old_acronyms$acronym,"Sub_basin","Sub_Basin"), to = c(old_acronyms$name,"Subbasin","Subbasin"),match_partial_entity = c(rep(F,length(old_acronyms$acronym)),T,T))
   
   new_extract_clean <- disambiguate(textnet_extract = readRDS("./new_extract"), from = c(new_acronyms$acronym,"Sub_basin","Sub_Basin"), to = c(new_acronyms$name,"Subbasin","Subbasin"), match_partial_entity = c(rep(F,length(new_acronyms$acronym)),T,T))

```

### Step 5: Get Network Attributes

A tool that generates an igraph object from the textnet_extract output is included in the package. Then various functions are invoked to create a network attribute table of common network-level attributes.


### Step 6: Find Top Features

Calculates the most common verbs and entities across the entire corpus of documents.

### Step 7: Generate Supernetwork

For best results, this should not be done without an adequate disambiguation in Step 4. A function is included that merges the edgelists and nodelists of all documents. If the same node name is mentioned in multiple documents, the node attributes associated with the highest total number of edges for that node name are preserved.


### Potential Analyses

End with collection of verb attributes, node attributes, edge incidences, and edge attributes. This can be analyzed through any number of tools, such as an ergm, or a tergm if one is reading in multiple versions of a document over time, to determine the probability of edge formation under certain conditions. Also have the ability to determine network-level attributes, which can be analyzed against exogenous metadata that has been collected separately by the researcher regarding the different documents and their real-world context.
