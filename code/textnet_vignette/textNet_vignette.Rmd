---
title: "textNet_vignette"
author: "E Zufall and T Scott"
date: "2023-09-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

While tools exist to generate networks based on co-occurrence of words within documents (such as the "textnets" package), there are limited tools for generating directed networks based on the syntactic relationships between entities within a sentence, for arbitrarily long documents. As many official documents, such as government planning documents or bills such as those studied in the social sciences, consist of thousands of pages, this type of replicable, automated network extraction, is extremely valuable. This package, textNet, is designed to enable just this. The textNet package facilitates the automated analysis and comparison of many documents, based on their respective network characteristics. It is flexible enough that any desired categor(ies) of entity, such as organizations, geopolitical entities, or dates, can be preserved.  

```{r cars}
summary(cars)
```

## Extracted Entity Network

The directed network generated by textNet represents the collection of all identified entities in the document, joined by edges signifying the verbs that connect them. The user can specify which categories of entities should be preserved. The output format is a list containing four data.tables: an edgelist, a nodelist, a verblist, and an appositive list. 

The edgelist includes edge attributes such as verb tense, any auxiliary verbs in the verb phrase, whether an open clausal complement (Universal Dependencies code "xcomp") is associated with the primary verb, whether any hedging words were detected in the sentence, and whether any negations were detected in the sentence. The edgelist by default returns both complete and incomplete edges. A complete edge includes a source, verb, and target. An incomplete edge includes either a source or a target, but not both, along with its associated verb. Incomplete edges convey information about which entities are commonly associated with different verbs, even though they do not reveal information about which other entities they are linked to in the network. These incomplete edges can be filtered out when converting the output into a network object, such as through the network package or the igraph package. The nodelist returns all entities of the desired types found in the document, regardless of whether they were found in the edgelist. Thus, the nodelist allows the presence of isolates to be documented. The verblist includes all of the verbs found in the document, along with verb attributes imported from VerbNet. This can be used to conduct analyses of certain verb classifications of interest. Finally, the appositive list is a table of entities that may be synonyms. This list is generated from entities whose universal dependency parsing labels as appositives, and whose head token points to another entity. These pairs are included in the table as potential synonyms. If this feature is used, cleaning and filtering by hand is recommended, as appositives can at times be misidentified by NLP tools such as spacy. An automated alternative we recommend is our find_acronym tool, which scans the entire document for acronyms defined parenthetically in-text and compiles them in a table.

This network is directed such that the entities that form the subject of the sentence are denoted as the "source" nodes, and the remaining entities are denoted as the "target" nodes. To identify whether each entity is a "source" or a "target", we use dependency parsing in the Universal Dependencies format, in which each token in a given sentence has an associated "syntactic head" token from which it is derived. Starting with each entity in the sentence, the chain of syntactic head tokens is traced back until either a subject or a verb is reached. If it reaches a subject first, the entity is considered a "source." If it reaches a verb first, it is considered a "target." 

To identify the subject, we search for the presence of at least one of the following subject tags: “nsubj” (nominal subject), “nsubjpass” (nominal subject – passive), “csubj” (clausal subject), “csubjpass” (clausal subject – passive), “agent”, and “expl” (expletive). To identify the object, we search for the presence of at least one of the following: “pobj” (object of preposition), “iobj” (indirect object), “dative”, “attr” (attribute), “dobj” (direct object), “oprd” (object predicate), “ccomp” (clausal complement), “xcomp” (open clausal complement), “acomp” (adjectival complement), or “pcomp” (complement of preposition).

If a subject token is reached first ("nsubj," "nsubjpass," "csubj," "csubjpass," "agent," or "expl"), this indicates that the original token is doing the verb action. That is, it serves some function related to the subject of the sentence. We designate this by tagging it “source,” since these types of relationships will be used to designate the “from” or “source” nodes in our directed network. If a verb token is reached first ("VERB" or "AUX"), this indicates that the verb action is occurring for or towards the original token, which we denote with the tag “target.” These tokens are potential “to” or “target” nodes in our directed network. Linking the two nodes is an edge representing the verb that connects them in the sentence. Due to the presence of tables, lists, or other anomalies in the dataset, it is possible that the supposed “sentence” has a head token trail that does not lead to a verb as is normatively the case. In these instances, the tokens whose trails terminate with a non-subject, non-verb token are assigned neither “source” nor “target” tags. Finally, an exception is made if an appositive token is reached first, since this indicates that the token in question is merely a synonym or restatement of an entity that is already described elsewhere in the sentence and should not be treated as a separate node. Tokens that lead to appositives are assigned neither “source” nor “target” tags, but are preserved as a separate appositive list. 

If a verb phrase in the edgelist does not have any sources, the sources associated with the head token of the verb phrase's main verb (that is, the verb phrase's parent verb) are adopted as sources of that verb phrase. As of Version 1.0, textNet does not do this recursively, to preserve performance optimization.

The textNet::textnet_extract() function returns the full list of open clausal complement lemmas associated with the main verb as an edge attribute: "xcomp_verb". The list of auxiliary verbs and their corresponding lemmas associated with the main verb, as well as the list of auxiliary verbs and corresponding lemmas associated with the open clausal complements linked to the main verb, are also included as edge attributes: "helper_token", "helper_lemma", "xcomp_helper_token", and "xcomp_helper_lemma", respectively.

The extraction function also detects hedging words and negations. The function textNet::textnet_extract() produces an edge attribute "has_hedge", which is T if there is a hedging auxiliary verb ("may","might","can","could") or main verb ("seem","appear","suggest","tend","assume","indicate","estimate","doubt","believe") in the verb phrase.

Tense is also detected. The six tenses tagged by Spacy in textNet::parse() are preserved by textNet::textnet_extract() as an edge attribute "head_verb_tense". This attribute can take on one of six values: "VB" (verb, base form), "VBD" (verb, past tense), "VBG" (verb, gerund or present participle), "VBN" (verb, past participle), "VBP" (verb, non-3rd person singular present), or "VBZ" (verb, 3rd person singular present). Additionally, an edge attribute "is_future" is generated by textNet::textnet_extract(), which is T if the verb phrase contains an xcomp, has the token "going" as a head verb, and a being verb token as an auxiliary verb (i.e. is of the form "going to <verb>") or contains one of the following auxiliary verbs: "shall","will","wo", or "'ll" (i.e. is of the form "will <verb>").


## How to Use textNet

The following example will walk through the steps of using the textNet package to generate a "before" and "after" network of organizations and people discussed in the Gravelly Ford Water District Groundwater Sustainability Plan, before and after it underwent revisions required by the California Department of Water Resources. 

### Step 1: Process PDFs

This is a wrapper for pdftools, that has the option of using pdf_text or OCR. A header/footer removal tool is also included. This tool is solely based on carriage returns in the first or last few lines of the document, so can sometimes remove portions of paragraphs. However, not removing headers or footers can lead to improper inclusion of header and footer material in sentences, artificially inflating the presence of nodes whose entity names are included in the header and footer. Therefore, the header/footer remover is included by default but can be turned off if the user has a preferred header/footer removal tool to use.

```{r pdf_clean, include=FALSE}

pdfs <- c(list.files(path = ".", pattern = "old.pdf", full.names = T), 
          list.files(path = ".", pattern = "new.pdf", full.names = T))
ocr <- F
maxchar <- 10000

old_new_text <- textNet::pdf_clean(pdfs, keep_pages=T, ocr=F, maxchar=10000, export_paths=NULL, return_to_memory=T, suppressWarn = F)
names(old_new_text) <- c("old","new")
```

### Step 2: Parse Text

This is a wrapper for spacyR, but another NLP tool can be used as long as the output conforms to spacy tagging standards: Universal Dependencies tags for part of speech, Penn Treebank tags for tags. The textnet_extract function expects a row for each token. The column names expected by textnet_extract are doc_id (a unique ID for each page), sentence_id (a unique ID for each sentence), token_id (a unique ID for each token), token (a string that is the token, generally a word), lemma (the canonical or dictionary form of the token), pos (a code referring to the token's part of speech, according to http://universaldependencies.org/u/pos/), tag (a code referring to the token's part of speech, according to Penn Treebank https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), head_token_id (a numeric ID referring to the token_id of the head token of the current row's token), dep_rel (the dependency label according to https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md), and entity (entity category defined by https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf, followed by "_B" if it is the first token in the entity or "_I" otherwise).

```{r parse, include=FALSE}
ret_path <- "/Users/elisemiller/miniconda3/envs/spacy_condaenv/bin/python"
pages <- unlist(old_new_text)
file_ids <- unlist(sapply(1:length(old_new_text), function(q) rep(names(old_new_text[q]),length(old_new_text[[q]]))))
old_new_parsed <- textNet::parse(ret_path, 
                                 keep_hyph_together = F, 
                                 phrases_to_concatenate = NA, 
                                 concatenator = "_", 
                                 pages, 
                                 file_ids, 
                                 parsed_filenames=c("old_parsed","new_parsed"), 
                                 parse_from_file = F, 
                                 cl= 1, 
                                 overwrite = T)
```

### Step 3: Extract Networks

This is the function that extracts the entity network. It reads in the result of parse() or another parsing tool with appropriate column names

```{r extract, include=FALSE}
nodeedge_filenames <- c("old_extract","new_extract")
for(m in 1:length(old_new_parsed)){
   textnet_extract(old_new_parsed[[m]],concatenator="_",file = nodeedge_filenames[m],cl=4,
                   keep_entities = c('ORG','GPE','PERSON'), 
                   return_to_memory=F, keep_incomplete_edges=T)
}
```

### Step 4: Consolidate Entity Synonyms
acronyms; known federal and state/regional agency names and abbreviations; document-specific abbreviations or nicknames such as "The Agency"  

```{r consolidate, include=FALSE}
   old_acronyms <- find_acronyms(old_new_text[[1]])
   new_acronyms <- find_acronyms(old_new_text[[2]])
   
   old_extract_clean <- disambiguate(from = old_acronyms$acronym, to = old_acronyms$name, textnet_extract = readRDS("./old_extract"))
   
   new_extract_clean <- disambiguate(from = new_acronyms$acronym, to = new_acronyms$name, textnet_extract = readRDS("new_extract"))

```
use find_acronyms to find acronyms

Generate your own context-specific list of agency names and abbreviations. A list of U.S. federal agency names and abbreviations is included in the package. 

### Step 5: Get Network Attributes

A tool that generates an igraph object from the textnet_extract output is included in the package. Then various functions are invoked to create a network attribute table of common network-level attributes.


### Step 6: Find Top Features

Calculates the most common verbs and entities across the entire corpus of documents.

### Step 7: Generate Supernetwork

For best results, this should not be done without an adequate disambiguation in Step 4. A function is included that merges the edgelists and nodelists of all documents. If the same node name is mentioned in multiple documents, the node attributes associated with the highest total number of edges for that node name are preserved.


### Potential Analyses

End with collection of verb attributes, node attributes, edge incidences, and edge attributes. This can be analyzed through any number of tools, such as an ergm, or a tergm if one is reading in multiple versions of a document over time, to determine the probability of edge formation under certain conditions. Also have the ability to determine network-level attributes, which can be analyzed against exogenous metadata that has been collected separately by the researcher regarding the different documents and their real-world context.
